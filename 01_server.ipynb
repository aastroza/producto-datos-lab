{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jIc0JvPWxvL"
   },
   "source": [
    "# Laboratorio: Implementación de un modelo de Machine Learning\n",
    "\n",
    "Esta experiencia de laboratorio se trata de implementar un modelo de ML para clasificación binaria usando datos tabulares. Para eso usaremos el modelo ya ajustado en el notebook [00_nyc-taxi-model.ipynb](https://colab.research.google.com/drive/1U71oPoyaaZJN2oniiHl_Re5W8T9LRZpA?usp=sharing). Para implementar el modelo como un servicio usaremos la popular biblioteca [`fastAPI`](https://fastapi.tiangolo.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWPBrZReWxvO"
   },
   "source": [
    "## Clasificación binaria de datos tabulares usando Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IScMsyeOWxvQ"
   },
   "source": [
    "### Creando la función predict_taxi_trip\n",
    "\n",
    "Vamos a crear el método `predict_taxi_trip` que toma como entradas un **array con los valores de las características del viaje** y un **umbral de confianza**. La función determinará si el viaje es de clase propina alta o propina baja, devolviendo un 1 o un 0 dependiendo del caso.\n",
    "\n",
    "La salida del modelo es un vector de probabilidades de pertenencia del viaje a alguna de las dos clases posibles. El último argumento de entrada a nuestra función (el nivel de confianza) será el umbral que dichas probabilidades deben superar para determinar que el viaje en cuestión si representa uno de propina alta. Por defecto `predict_taxi_trip` usa el valor 0.5 para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1656110635634,
     "user": {
      "displayName": "ALONSO ASTROZA TAGLE",
      "userId": "10910938541244248966"
     },
     "user_tz": 240
    },
    "id": "VFWjz_dDWxvR"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "rfc = joblib.load(\"./model/random_forest.joblib\")\n",
    "\n",
    "def predict_taxi_trip(features_trip, confidence=0.5):\n",
    "    \"\"\"Recibe un vector de características de un viaje en taxi en NYC y predice \n",
    "       si el pasajero dejará o no una propina alta.\n",
    "\n",
    "    Argumentos:\n",
    "        features_trip (array): Características del viaje, vector de tamaño 11.\n",
    "        confidence (float, opcional): Nivel de confianza. Por defecto es 0.5.\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_value = rfc.predict_proba(features_trip.reshape(1, -1))[0][1]\n",
    "    if pred_value >= confidence:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlYkci_uWxvS"
   },
   "source": [
    "Probemos sobre un viaje de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1656110637954,
     "user": {
      "displayName": "ALONSO ASTROZA TAGLE",
      "userId": "10910938541244248966"
     },
     "user_tz": 240
    },
    "id": "f46Ot9Du4dBZ"
   },
   "outputs": [],
   "source": [
    "features_trip = np.array([5.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.7000000e+01,\n",
    "       1.0000000e+00, 2.5999999e+00, 7.7700000e+02, 3.3462034e-03,\n",
    "       1.4500000e+02, 7.0000000e+00, 1.0000000e+00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1656110662902,
     "user": {
      "displayName": "ALONSO ASTROZA TAGLE",
      "userId": "10910938541244248966"
     },
     "user_tz": 240
    },
    "id": "yti9Aa5vWxvS",
    "outputId": "06e8af66-e5d6-4a35-cc50-e4c39a9752ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\personal\\producto-datos-lab\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_taxi_trip(features_trip, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cambiando el nivel de confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\personal\\producto-datos-lab\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_taxi_trip(features_trip, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\personal\\producto-datos-lab\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_taxi_trip(features_trip, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s5GeOywWxvU"
   },
   "source": [
    "## Implementando el modelo usando fastAPI\n",
    "\n",
    "\n",
    "### Poniendo nuestro modelo de clasificación de viajes en un servidor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbE0cOhEWxvU"
   },
   "source": [
    "### Conceptos importantes\n",
    "\n",
    "#### Modelo Cliente-Servidor\n",
    "\n",
    "Cuando hablamos de **implementar**, lo que usualmente se quiere decir es que vamos a poner todo el software necesario para realizar predicciones en un `server` (servidor). De esta forma un `client` (cliente) puede interactuar con el modelo enviando `requests` (solicitudes) al servidor. \n",
    "\n",
    "Lo importante entonces es que el modelo de Machine Learning vive en un servidor esperando por clientes que le envíen solicitudes de predicciones. El cliente tiene que entregarle toda la información necesaria que el modelo necesita para poder hacer una predicción. Tengamos en mente que es común acumular más de una predicción en una misma solicitud. El servidor usará la información que le proporcionen para devolver predicciones al cliente, el cual puede usarlas a su antojo.\n",
    "\n",
    "Empecemos creando una instancia de la clase `FastAPI`:\n",
    "\n",
    "```python\n",
    "app = FastAPI()\n",
    "```\n",
    "\n",
    "El siguiente paso es usar esa instancia para crear endpoints que manejarán la lógica para hacer predicciones. Una vez que todo el código está listo para correr el servidor solo hay que usar el siguiente comando:\n",
    "\n",
    "```python\n",
    "uvicorn.run(app)\n",
    "```\n",
    "\n",
    "La API está construida usando código de fastAPI pero \"servirla\" se hace mediante [`uvicorn`](https://www.uvicorn.org/), que es una Asynchronous Server Gateway Interface (ASGI) de muy rápida implementación. Ambas tecnologías están super conectadas pero no necesitamos entender los detalles técnicos. Sólo hay que tener en cuenta que es uvicorn el que se encarga de servir el código.\n",
    "\n",
    "#### Endpoints\n",
    "\n",
    "Podemos hospedar varios modelos de Machine Learning en el mismo servidor. Para esto podemos asignarles un `endpoint` diferente a cada modelo para que sepamos siempre cuál de los modelos estamos usando. Un endpoint se representa como un patrón en la `URL`. Por ejemplo si tenemos un sitio que se llama `misupermodelo.com` también podríamos tener tres diferentes modelos en los siguientes endpoints:\n",
    "\n",
    "- `misupermodelo.com/contador-autos/`\n",
    "- `misupermodelo.com/predictor-serie-de-tiempo/`\n",
    "- `misupermodelo.com/recomendador-de-autos/`\n",
    "\n",
    "Cada modelo llevaría a cabo la tarea que el patrón de la URL indica.\n",
    "\n",
    "En fastAPI podemos definir un endpoint creando una función que se encargue de manejar la lógica que corresponde. Además se incluye un [decorador](https://www.python.org/dev/peps/pep-0318/) con una función que contiene la información de que método HTTP está permitido y cuál es el patrón de la URL que se usará para el endpoint en cuestión.\n",
    "\n",
    "El siguiente ejemplo muestra como generar una solicitud HTTP GET en el endpoint endpoint \"/mi-endpoint\":\n",
    "\n",
    "```python\n",
    "@app.get(\"/mi-endpoint\")\n",
    "def handle_endpoint():\n",
    "    ...\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "#### Solicitudes HTTP\n",
    "\n",
    "El cliente y el servidor se comunican entre sí a través de un protocolo llamado `HTTP`. El concepto clave es que la comunicación entre cliente y servidor usa ciertos verbos que denotan acciones. Dos verbos comunes son:\n",
    "\n",
    "- `GET` -> Obtiene información del servidor.\n",
    "- `POST` -> Entrega información al servidor, la cual se usa para responder.\n",
    "\n",
    "Si el cliente hace un `GET request` a un endpoint el servidor entregará información del endpoint sin necesidad de que le proporcionemos información adicional. En el caso de un `POST request` le estamos diciendo de manera explícita al servidor que le entregaremos información para que la procese de alguna forma.\n",
    "\n",
    "Para interactuar con modelos de Machine Learning que estén viviendo en endpoints usualmente hacemos un `POST request` ya que siempre necesitaremos entregarle información para que realice predicciones.\n",
    "\n",
    "Así luce un POST request:\n",
    "\n",
    "```python\n",
    "@app.post(\"/mi-otro-endpoint\")\n",
    "def handle_other_endpoint(param1: int, param2: str):\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "Para POST requests, la función debe contener parámetros. En contraste con un GET, las solicitudes POST esperan que el cliente les entregue alguna información. En este ejemplo proveerá un entero y un string.\n",
    "\n",
    "\n",
    "### ¿Por qué fastAPI?\n",
    "\n",
    "Con fastAPI podemos crear servidores web par hospedar modelos de manera muy sencilla. Adicionalmente la plataforma es extremadamente rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "error",
     "timestamp": 1656110785852,
     "user": {
      "displayName": "ALONSO ASTROZA TAGLE",
      "userId": "10910938541244248966"
     },
     "user_tz": 240
    },
    "id": "DG-PuMIYWxvV",
    "outputId": "767cbba2-fed7-4a72-bfe0-662741509ed3"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import uvicorn\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "from enum import Enum\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ORNAXD0dWxvV"
   },
   "outputs": [],
   "source": [
    "# Asignamos una instancia de la clase FastAPI a la variable \"app\".\n",
    "# Interacturaremos con la API usando este elemento.\n",
    "app = FastAPI(title='Implementando un modelo de Machine Learning usando FastAPI')\n",
    "\n",
    "# Creamos una clase para el vector de features de entrada\n",
    "class Item(BaseModel):\n",
    "    pickup_weekday: float\n",
    "    pickup_hour: float\n",
    "    work_hours: float\n",
    "    pickup_minute: float\n",
    "    passenger_count: float\n",
    "    trip_distance: float\n",
    "    trip_time: float\n",
    "    trip_speed: float\n",
    "    PULocationID: float\n",
    "    DOLocationID: float\n",
    "    RatecodeID: float\n",
    "\n",
    "# Usando @app.get(\"/\") definimos un método GET para el endpoint / (que sería como el \"home\").\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return \"¡Felicitaciones! Tu API está funcionando según lo esperado. Anda ahora a http://localhost:8000/docs.\"\n",
    "\n",
    "\n",
    "# Este endpoint maneja la lógica necesaria para clasificar.\n",
    "# Requiere como entrada el vector de características del viaje y el umbral de confianza para la clasificación.\n",
    "@app.post(\"/predict\") \n",
    "def prediction(item: Item, confidence: float):\n",
    "\n",
    "    \n",
    "    # 1. Correr el modelo de clasificación\n",
    "    features_trip = np.array([item.pickup_weekday, item.pickup_hour, item.work_hours, item.pickup_minute, item.passenger_count, item.trip_distance,\n",
    "                    item.trip_time, item.trip_speed, item.PULocationID, item.DOLocationID, item.RatecodeID])\n",
    "    pred = predict_taxi_trip(features_trip, confidence)\n",
    "    \n",
    "    # 2. Transmitir la respuesta de vuelta al cliente\n",
    "\n",
    "    # Retornar el resultado de la predicción\n",
    "    return {'predicted_class': pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SDCwso8WxvW"
   },
   "source": [
    "¡Corriendo la celda que viene echaremos a andar el servidor!\n",
    "\n",
    "Esto causará que el notebook se bloquee (no podremos correr más celdas) hasta que interrumpamos de forma manual el kernel. Podemos hacer eso haciendo click en la pestaña `Kernel` y luego `Interrupt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stbOFHfDWxvW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [28492]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58297 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58297 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:58297 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58297 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:58414 - \"POST /predict?confidence=0.5 HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\personal\\producto-datos-lab\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Esto deja correr al servidor en un ambiente interactivo como un Jupyter notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Donde se hospedará el servidor\n",
    "host = \"127.0.0.1\"\n",
    "\n",
    "# ¡Iniciemos el servidor!    \n",
    "uvicorn.run(app, host=host, port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heHHxlhiWxvX"
   },
   "source": [
    "¡El servidor está corriendo! Vamos a [http://localhost:8000/](http://localhost:8000/) para verlo en acción.\n",
    "\n",
    "**Probemos enviando un viaje de ejemplo** y veamos como nuestra API es capaz de clasificarlo y retornar la etiquetas del tipo de viaje. **Podemos hacer eso visitando [http://localhost:8000/docs](http://localhost:8000/docs) para abrir un cliente que viene dentro de fastAPI.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmDESPfqWxvX"
   },
   "source": [
    "Si hacemos click en el endpoint `/predict` se verán más opciones. Para probar el servidor hay que usar el botón **Try it out**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1606pp8VWxvX"
   },
   "source": [
    "Podemos elegir un nivel de confianza usando el campo **confidence** y representar un **viaje** completando el diccionario del **Request body**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPNz18AhWxvX"
   },
   "source": [
    "## Consumiendo el modelo desde otro cliente\n",
    "\n",
    "Es genial que fastAPI permita interactuar con la API por medio del cliente que tiene incorporado. Pero debemos aprender como usar la API desde cualquier tipo de código, no necesariamente con una interfaz.\n",
    "\n",
    "Para eso iremos al siguiente notebook donde implementaremos un cliente básico en Python. Para esto **debemos dejar corriendo el servidor (no paremos el kernel ni cerremos esta ventana)** y abramos el notebook `02_client.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_server.ipynb",
   "provenance": [
    {
     "file_id": "19IyNU-ulwpU7YRhXW3rDVU_-rDjtkZBW",
     "timestamp": 1656109671777
    }
   ]
  },
  "kernelspec": {
   "display_name": "producto-datos-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
